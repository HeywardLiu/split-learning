{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For synchronous execution of CPU and GPU\n",
    "client_device = torch.device(\"cuda:2\")\n",
    "server_device = torch.device(\"cuda:1\")\n",
    "model_device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2023-10-26 09:30:09,847] [INFO] [real_accelerator.py:133:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "MixFormerSequentialConfig {\n",
      "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"MixFormerSequentialForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"mixformer-sequential\",\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 32,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.29.2\",\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "MixFormerSequentialConfig {\n",
      "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"MixFormerSequentialForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"mixformer-sequential\",\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 32,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.29.2\",\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "from pipe_mixformer_sequential import ClientSideMixFormerSequentialForCausalLM, ServerSideMixFormerSequentialForCausalLM\n",
    "from configuration_mixformer_sequential import MixFormerSequentialConfig\n",
    "from modeling_mixformer_sequential import MixFormerSequentialForCausalLM\n",
    "config = AutoConfig.from_pretrained(\"microsoft/phi-1_5\", trust_remote_code=True)\n",
    "# config = MixFormerSequentialConfig(n_layer=25)\n",
    "client_model = ClientSideMixFormerSequentialForCausalLM(config)\n",
    "server_model = ServerSideMixFormerSequentialForCausalLM(config)\n",
    "model = MixFormerSequentialForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MixFormerSequentialConfig {\n",
      "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"MixFormerSequentialForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"mixformer-sequential\",\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 32,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.29.2\",\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n",
      "MixFormerSequentialConfig {\n",
      "  \"_name_or_path\": \"microsoft/phi-1_5\",\n",
      "  \"activation_function\": \"gelu_new\",\n",
      "  \"architectures\": [\n",
      "    \"MixFormerSequentialForCausalLM\"\n",
      "  ],\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/phi-1_5--configuration_mixformer_sequential.MixFormerSequentialConfig\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/phi-1_5--modeling_mixformer_sequential.MixFormerSequentialForCausalLM\"\n",
      "  },\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"model_type\": \"mixformer-sequential\",\n",
      "  \"n_embd\": 2048,\n",
      "  \"n_head\": 32,\n",
      "  \"n_inner\": null,\n",
      "  \"n_layer\": 24,\n",
      "  \"n_positions\": 2048,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rotary_dim\": 32,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"float16\",\n",
      "  \"transformers_version\": \"4.29.2\",\n",
      "  \"vocab_size\": 51200\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from pipe_mixformer_sequential import ClientSideMixFormerSequentialForCausalLM, ServerSideMixFormerSequentialForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "model_name = (\"gpt2\", \"microsoft/phi-1_5\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_name[1], \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name[1],\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "client_model = ClientSideMixFormerSequentialForCausalLM(model.config)\n",
    "server_model = ServerSideMixFormerSequentialForCausalLM(model.config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_fp32_layers(model):\n",
    "    fp32_layers=[]\n",
    "    for k, v in model.state_dict().items():\n",
    "        print(f\"{k}: {v.dtype}\")\n",
    "        if(v.dtype == torch.float32):\n",
    "            fp32_layers.append(k)\n",
    "    return fp32_layers\n",
    "\n",
    "\n",
    "def load_pretrained_weights(sub_model, model, fp32_layers):    \n",
    "    np_param={}\n",
    "    for module_name, param in model.state_dict().items():\n",
    "        np_param[module_name] = param.numpy().astype(np.float16)\n",
    "        \n",
    "    # sub_model = sub_model.to(torch.float16)\n",
    "    for module_name, param in sub_model.state_dict().items():\n",
    "        if(model_name not in fp32_layers):\n",
    "            sub_model.state_dict()[module_name] = sub_model.state_dict()[module_name].to(torch.float16)\n",
    "        \n",
    "        sub_model.state_dict()[module_name].data.copy_(torch.from_numpy(np_param[module_name]))\n",
    "        sub_model.state_dict()[module_name]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.wte.weight: torch.float16\n",
      "layers.1.ln.weight: torch.float16\n",
      "layers.1.ln.bias: torch.float16\n",
      "layers.1.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.1.mixer.Wqkv.weight: torch.float16\n",
      "layers.1.mixer.Wqkv.bias: torch.float16\n",
      "layers.1.mixer.out_proj.weight: torch.float16\n",
      "layers.1.mixer.out_proj.bias: torch.float16\n",
      "layers.1.mlp.fc1.weight: torch.float16\n",
      "layers.1.mlp.fc1.bias: torch.float16\n",
      "layers.1.mlp.fc2.weight: torch.float16\n",
      "layers.1.mlp.fc2.bias: torch.float16\n",
      "layers.2.ln.weight: torch.float16\n",
      "layers.2.ln.bias: torch.float16\n",
      "layers.2.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.2.mixer.Wqkv.weight: torch.float16\n",
      "layers.2.mixer.Wqkv.bias: torch.float16\n",
      "layers.2.mixer.out_proj.weight: torch.float16\n",
      "layers.2.mixer.out_proj.bias: torch.float16\n",
      "layers.2.mlp.fc1.weight: torch.float16\n",
      "layers.2.mlp.fc1.bias: torch.float16\n",
      "layers.2.mlp.fc2.weight: torch.float16\n",
      "layers.2.mlp.fc2.bias: torch.float16\n",
      "layers.3.ln.weight: torch.float16\n",
      "layers.3.ln.bias: torch.float16\n",
      "layers.3.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.3.mixer.Wqkv.weight: torch.float16\n",
      "layers.3.mixer.Wqkv.bias: torch.float16\n",
      "layers.3.mixer.out_proj.weight: torch.float16\n",
      "layers.3.mixer.out_proj.bias: torch.float16\n",
      "layers.3.mlp.fc1.weight: torch.float16\n",
      "layers.3.mlp.fc1.bias: torch.float16\n",
      "layers.3.mlp.fc2.weight: torch.float16\n",
      "layers.3.mlp.fc2.bias: torch.float16\n",
      "layers.4.ln.weight: torch.float16\n",
      "layers.4.ln.bias: torch.float16\n",
      "layers.4.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.4.mixer.Wqkv.weight: torch.float16\n",
      "layers.4.mixer.Wqkv.bias: torch.float16\n",
      "layers.4.mixer.out_proj.weight: torch.float16\n",
      "layers.4.mixer.out_proj.bias: torch.float16\n",
      "layers.4.mlp.fc1.weight: torch.float16\n",
      "layers.4.mlp.fc1.bias: torch.float16\n",
      "layers.4.mlp.fc2.weight: torch.float16\n",
      "layers.4.mlp.fc2.bias: torch.float16\n",
      "layers.5.ln.weight: torch.float16\n",
      "layers.5.ln.bias: torch.float16\n",
      "layers.5.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.5.mixer.Wqkv.weight: torch.float16\n",
      "layers.5.mixer.Wqkv.bias: torch.float16\n",
      "layers.5.mixer.out_proj.weight: torch.float16\n",
      "layers.5.mixer.out_proj.bias: torch.float16\n",
      "layers.5.mlp.fc1.weight: torch.float16\n",
      "layers.5.mlp.fc1.bias: torch.float16\n",
      "layers.5.mlp.fc2.weight: torch.float16\n",
      "layers.5.mlp.fc2.bias: torch.float16\n",
      "layers.6.ln.weight: torch.float16\n",
      "layers.6.ln.bias: torch.float16\n",
      "layers.6.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.6.mixer.Wqkv.weight: torch.float16\n",
      "layers.6.mixer.Wqkv.bias: torch.float16\n",
      "layers.6.mixer.out_proj.weight: torch.float16\n",
      "layers.6.mixer.out_proj.bias: torch.float16\n",
      "layers.6.mlp.fc1.weight: torch.float16\n",
      "layers.6.mlp.fc1.bias: torch.float16\n",
      "layers.6.mlp.fc2.weight: torch.float16\n",
      "layers.6.mlp.fc2.bias: torch.float16\n",
      "layers.7.ln.weight: torch.float16\n",
      "layers.7.ln.bias: torch.float16\n",
      "layers.7.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.7.mixer.Wqkv.weight: torch.float16\n",
      "layers.7.mixer.Wqkv.bias: torch.float16\n",
      "layers.7.mixer.out_proj.weight: torch.float16\n",
      "layers.7.mixer.out_proj.bias: torch.float16\n",
      "layers.7.mlp.fc1.weight: torch.float16\n",
      "layers.7.mlp.fc1.bias: torch.float16\n",
      "layers.7.mlp.fc2.weight: torch.float16\n",
      "layers.7.mlp.fc2.bias: torch.float16\n",
      "layers.8.ln.weight: torch.float16\n",
      "layers.8.ln.bias: torch.float16\n",
      "layers.8.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.8.mixer.Wqkv.weight: torch.float16\n",
      "layers.8.mixer.Wqkv.bias: torch.float16\n",
      "layers.8.mixer.out_proj.weight: torch.float16\n",
      "layers.8.mixer.out_proj.bias: torch.float16\n",
      "layers.8.mlp.fc1.weight: torch.float16\n",
      "layers.8.mlp.fc1.bias: torch.float16\n",
      "layers.8.mlp.fc2.weight: torch.float16\n",
      "layers.8.mlp.fc2.bias: torch.float16\n",
      "layers.9.ln.weight: torch.float16\n",
      "layers.9.ln.bias: torch.float16\n",
      "layers.9.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.9.mixer.Wqkv.weight: torch.float16\n",
      "layers.9.mixer.Wqkv.bias: torch.float16\n",
      "layers.9.mixer.out_proj.weight: torch.float16\n",
      "layers.9.mixer.out_proj.bias: torch.float16\n",
      "layers.9.mlp.fc1.weight: torch.float16\n",
      "layers.9.mlp.fc1.bias: torch.float16\n",
      "layers.9.mlp.fc2.weight: torch.float16\n",
      "layers.9.mlp.fc2.bias: torch.float16\n",
      "layers.10.ln.weight: torch.float16\n",
      "layers.10.ln.bias: torch.float16\n",
      "layers.10.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.10.mixer.Wqkv.weight: torch.float16\n",
      "layers.10.mixer.Wqkv.bias: torch.float16\n",
      "layers.10.mixer.out_proj.weight: torch.float16\n",
      "layers.10.mixer.out_proj.bias: torch.float16\n",
      "layers.10.mlp.fc1.weight: torch.float16\n",
      "layers.10.mlp.fc1.bias: torch.float16\n",
      "layers.10.mlp.fc2.weight: torch.float16\n",
      "layers.10.mlp.fc2.bias: torch.float16\n",
      "layers.11.ln.weight: torch.float16\n",
      "layers.11.ln.bias: torch.float16\n",
      "layers.11.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.11.mixer.Wqkv.weight: torch.float16\n",
      "layers.11.mixer.Wqkv.bias: torch.float16\n",
      "layers.11.mixer.out_proj.weight: torch.float16\n",
      "layers.11.mixer.out_proj.bias: torch.float16\n",
      "layers.11.mlp.fc1.weight: torch.float16\n",
      "layers.11.mlp.fc1.bias: torch.float16\n",
      "layers.11.mlp.fc2.weight: torch.float16\n",
      "layers.11.mlp.fc2.bias: torch.float16\n",
      "layers.12.ln.weight: torch.float16\n",
      "layers.12.ln.bias: torch.float16\n",
      "layers.12.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.12.mixer.Wqkv.weight: torch.float16\n",
      "layers.12.mixer.Wqkv.bias: torch.float16\n",
      "layers.12.mixer.out_proj.weight: torch.float16\n",
      "layers.12.mixer.out_proj.bias: torch.float16\n",
      "layers.12.mlp.fc1.weight: torch.float16\n",
      "layers.12.mlp.fc1.bias: torch.float16\n",
      "layers.12.mlp.fc2.weight: torch.float16\n",
      "layers.12.mlp.fc2.bias: torch.float16\n",
      "layers.13.ln.weight: torch.float16\n",
      "layers.13.ln.bias: torch.float16\n",
      "layers.13.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.13.mixer.Wqkv.weight: torch.float16\n",
      "layers.13.mixer.Wqkv.bias: torch.float16\n",
      "layers.13.mixer.out_proj.weight: torch.float16\n",
      "layers.13.mixer.out_proj.bias: torch.float16\n",
      "layers.13.mlp.fc1.weight: torch.float16\n",
      "layers.13.mlp.fc1.bias: torch.float16\n",
      "layers.13.mlp.fc2.weight: torch.float16\n",
      "layers.13.mlp.fc2.bias: torch.float16\n",
      "layers.14.ln.weight: torch.float16\n",
      "layers.14.ln.bias: torch.float16\n",
      "layers.14.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.14.mixer.Wqkv.weight: torch.float16\n",
      "layers.14.mixer.Wqkv.bias: torch.float16\n",
      "layers.14.mixer.out_proj.weight: torch.float16\n",
      "layers.14.mixer.out_proj.bias: torch.float16\n",
      "layers.14.mlp.fc1.weight: torch.float16\n",
      "layers.14.mlp.fc1.bias: torch.float16\n",
      "layers.14.mlp.fc2.weight: torch.float16\n",
      "layers.14.mlp.fc2.bias: torch.float16\n",
      "layers.15.ln.weight: torch.float16\n",
      "layers.15.ln.bias: torch.float16\n",
      "layers.15.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.15.mixer.Wqkv.weight: torch.float16\n",
      "layers.15.mixer.Wqkv.bias: torch.float16\n",
      "layers.15.mixer.out_proj.weight: torch.float16\n",
      "layers.15.mixer.out_proj.bias: torch.float16\n",
      "layers.15.mlp.fc1.weight: torch.float16\n",
      "layers.15.mlp.fc1.bias: torch.float16\n",
      "layers.15.mlp.fc2.weight: torch.float16\n",
      "layers.15.mlp.fc2.bias: torch.float16\n",
      "layers.16.ln.weight: torch.float16\n",
      "layers.16.ln.bias: torch.float16\n",
      "layers.16.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.16.mixer.Wqkv.weight: torch.float16\n",
      "layers.16.mixer.Wqkv.bias: torch.float16\n",
      "layers.16.mixer.out_proj.weight: torch.float16\n",
      "layers.16.mixer.out_proj.bias: torch.float16\n",
      "layers.16.mlp.fc1.weight: torch.float16\n",
      "layers.16.mlp.fc1.bias: torch.float16\n",
      "layers.16.mlp.fc2.weight: torch.float16\n",
      "layers.16.mlp.fc2.bias: torch.float16\n",
      "layers.17.ln.weight: torch.float16\n",
      "layers.17.ln.bias: torch.float16\n",
      "layers.17.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.17.mixer.Wqkv.weight: torch.float16\n",
      "layers.17.mixer.Wqkv.bias: torch.float16\n",
      "layers.17.mixer.out_proj.weight: torch.float16\n",
      "layers.17.mixer.out_proj.bias: torch.float16\n",
      "layers.17.mlp.fc1.weight: torch.float16\n",
      "layers.17.mlp.fc1.bias: torch.float16\n",
      "layers.17.mlp.fc2.weight: torch.float16\n",
      "layers.17.mlp.fc2.bias: torch.float16\n",
      "layers.18.ln.weight: torch.float16\n",
      "layers.18.ln.bias: torch.float16\n",
      "layers.18.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.18.mixer.Wqkv.weight: torch.float16\n",
      "layers.18.mixer.Wqkv.bias: torch.float16\n",
      "layers.18.mixer.out_proj.weight: torch.float16\n",
      "layers.18.mixer.out_proj.bias: torch.float16\n",
      "layers.18.mlp.fc1.weight: torch.float16\n",
      "layers.18.mlp.fc1.bias: torch.float16\n",
      "layers.18.mlp.fc2.weight: torch.float16\n",
      "layers.18.mlp.fc2.bias: torch.float16\n",
      "layers.19.ln.weight: torch.float16\n",
      "layers.19.ln.bias: torch.float16\n",
      "layers.19.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.19.mixer.Wqkv.weight: torch.float16\n",
      "layers.19.mixer.Wqkv.bias: torch.float16\n",
      "layers.19.mixer.out_proj.weight: torch.float16\n",
      "layers.19.mixer.out_proj.bias: torch.float16\n",
      "layers.19.mlp.fc1.weight: torch.float16\n",
      "layers.19.mlp.fc1.bias: torch.float16\n",
      "layers.19.mlp.fc2.weight: torch.float16\n",
      "layers.19.mlp.fc2.bias: torch.float16\n",
      "layers.20.ln.weight: torch.float16\n",
      "layers.20.ln.bias: torch.float16\n",
      "layers.20.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.20.mixer.Wqkv.weight: torch.float16\n",
      "layers.20.mixer.Wqkv.bias: torch.float16\n",
      "layers.20.mixer.out_proj.weight: torch.float16\n",
      "layers.20.mixer.out_proj.bias: torch.float16\n",
      "layers.20.mlp.fc1.weight: torch.float16\n",
      "layers.20.mlp.fc1.bias: torch.float16\n",
      "layers.20.mlp.fc2.weight: torch.float16\n",
      "layers.20.mlp.fc2.bias: torch.float16\n",
      "layers.21.ln.weight: torch.float16\n",
      "layers.21.ln.bias: torch.float16\n",
      "layers.21.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.21.mixer.Wqkv.weight: torch.float16\n",
      "layers.21.mixer.Wqkv.bias: torch.float16\n",
      "layers.21.mixer.out_proj.weight: torch.float16\n",
      "layers.21.mixer.out_proj.bias: torch.float16\n",
      "layers.21.mlp.fc1.weight: torch.float16\n",
      "layers.21.mlp.fc1.bias: torch.float16\n",
      "layers.21.mlp.fc2.weight: torch.float16\n",
      "layers.21.mlp.fc2.bias: torch.float16\n",
      "layers.22.ln.weight: torch.float16\n",
      "layers.22.ln.bias: torch.float16\n",
      "layers.22.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.22.mixer.Wqkv.weight: torch.float16\n",
      "layers.22.mixer.Wqkv.bias: torch.float16\n",
      "layers.22.mixer.out_proj.weight: torch.float16\n",
      "layers.22.mixer.out_proj.bias: torch.float16\n",
      "layers.22.mlp.fc1.weight: torch.float16\n",
      "layers.22.mlp.fc1.bias: torch.float16\n",
      "layers.22.mlp.fc2.weight: torch.float16\n",
      "layers.22.mlp.fc2.bias: torch.float16\n",
      "layers.23.ln.weight: torch.float16\n",
      "layers.23.ln.bias: torch.float16\n",
      "layers.23.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.23.mixer.Wqkv.weight: torch.float16\n",
      "layers.23.mixer.Wqkv.bias: torch.float16\n",
      "layers.23.mixer.out_proj.weight: torch.float16\n",
      "layers.23.mixer.out_proj.bias: torch.float16\n",
      "layers.23.mlp.fc1.weight: torch.float16\n",
      "layers.23.mlp.fc1.bias: torch.float16\n",
      "layers.23.mlp.fc2.weight: torch.float16\n",
      "layers.23.mlp.fc2.bias: torch.float16\n",
      "layers.24.ln.weight: torch.float16\n",
      "layers.24.ln.bias: torch.float16\n",
      "layers.24.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.24.mixer.Wqkv.weight: torch.float16\n",
      "layers.24.mixer.Wqkv.bias: torch.float16\n",
      "layers.24.mixer.out_proj.weight: torch.float16\n",
      "layers.24.mixer.out_proj.bias: torch.float16\n",
      "layers.24.mlp.fc1.weight: torch.float16\n",
      "layers.24.mlp.fc1.bias: torch.float16\n",
      "layers.24.mlp.fc2.weight: torch.float16\n",
      "layers.24.mlp.fc2.bias: torch.float16\n",
      "layers.25.ln.weight: torch.float16\n",
      "layers.25.ln.bias: torch.float16\n",
      "layers.25.linear.weight: torch.float16\n",
      "layers.25.linear.bias: torch.float16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "fp32_layers = get_fp32_layers(model)\n",
    "load_pretrained_weights(client_model, model, fp32_layers)\n",
    "load_pretrained_weights(server_model, model, fp32_layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.wte.weight: torch.float32\n",
      "layers.1.ln.weight: torch.float32\n",
      "layers.1.ln.bias: torch.float32\n",
      "layers.1.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.1.mixer.Wqkv.weight: torch.float32\n",
      "layers.1.mixer.Wqkv.bias: torch.float32\n",
      "layers.1.mixer.out_proj.weight: torch.float32\n",
      "layers.1.mixer.out_proj.bias: torch.float32\n",
      "layers.1.mlp.fc1.weight: torch.float32\n",
      "layers.1.mlp.fc1.bias: torch.float32\n",
      "layers.1.mlp.fc2.weight: torch.float32\n",
      "layers.1.mlp.fc2.bias: torch.float32\n",
      "layers.2.ln.weight: torch.float32\n",
      "layers.2.ln.bias: torch.float32\n",
      "layers.2.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.2.mixer.Wqkv.weight: torch.float32\n",
      "layers.2.mixer.Wqkv.bias: torch.float32\n",
      "layers.2.mixer.out_proj.weight: torch.float32\n",
      "layers.2.mixer.out_proj.bias: torch.float32\n",
      "layers.2.mlp.fc1.weight: torch.float32\n",
      "layers.2.mlp.fc1.bias: torch.float32\n",
      "layers.2.mlp.fc2.weight: torch.float32\n",
      "layers.2.mlp.fc2.bias: torch.float32\n",
      "layers.3.ln.weight: torch.float32\n",
      "layers.3.ln.bias: torch.float32\n",
      "layers.3.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.3.mixer.Wqkv.weight: torch.float32\n",
      "layers.3.mixer.Wqkv.bias: torch.float32\n",
      "layers.3.mixer.out_proj.weight: torch.float32\n",
      "layers.3.mixer.out_proj.bias: torch.float32\n",
      "layers.3.mlp.fc1.weight: torch.float32\n",
      "layers.3.mlp.fc1.bias: torch.float32\n",
      "layers.3.mlp.fc2.weight: torch.float32\n",
      "layers.3.mlp.fc2.bias: torch.float32\n",
      "layers.4.ln.weight: torch.float32\n",
      "layers.4.ln.bias: torch.float32\n",
      "layers.4.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.4.mixer.Wqkv.weight: torch.float32\n",
      "layers.4.mixer.Wqkv.bias: torch.float32\n",
      "layers.4.mixer.out_proj.weight: torch.float32\n",
      "layers.4.mixer.out_proj.bias: torch.float32\n",
      "layers.4.mlp.fc1.weight: torch.float32\n",
      "layers.4.mlp.fc1.bias: torch.float32\n",
      "layers.4.mlp.fc2.weight: torch.float32\n",
      "layers.4.mlp.fc2.bias: torch.float32\n",
      "layers.5.ln.weight: torch.float32\n",
      "layers.5.ln.bias: torch.float32\n",
      "layers.5.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.5.mixer.Wqkv.weight: torch.float32\n",
      "layers.5.mixer.Wqkv.bias: torch.float32\n",
      "layers.5.mixer.out_proj.weight: torch.float32\n",
      "layers.5.mixer.out_proj.bias: torch.float32\n",
      "layers.5.mlp.fc1.weight: torch.float32\n",
      "layers.5.mlp.fc1.bias: torch.float32\n",
      "layers.5.mlp.fc2.weight: torch.float32\n",
      "layers.5.mlp.fc2.bias: torch.float32\n",
      "layers.6.ln.weight: torch.float32\n",
      "layers.6.ln.bias: torch.float32\n",
      "layers.6.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.6.mixer.Wqkv.weight: torch.float32\n",
      "layers.6.mixer.Wqkv.bias: torch.float32\n",
      "layers.6.mixer.out_proj.weight: torch.float32\n",
      "layers.6.mixer.out_proj.bias: torch.float32\n",
      "layers.6.mlp.fc1.weight: torch.float32\n",
      "layers.6.mlp.fc1.bias: torch.float32\n",
      "layers.6.mlp.fc2.weight: torch.float32\n",
      "layers.6.mlp.fc2.bias: torch.float32\n",
      "layers.7.ln.weight: torch.float32\n",
      "layers.7.ln.bias: torch.float32\n",
      "layers.7.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.7.mixer.Wqkv.weight: torch.float32\n",
      "layers.7.mixer.Wqkv.bias: torch.float32\n",
      "layers.7.mixer.out_proj.weight: torch.float32\n",
      "layers.7.mixer.out_proj.bias: torch.float32\n",
      "layers.7.mlp.fc1.weight: torch.float32\n",
      "layers.7.mlp.fc1.bias: torch.float32\n",
      "layers.7.mlp.fc2.weight: torch.float32\n",
      "layers.7.mlp.fc2.bias: torch.float32\n",
      "layers.8.ln.weight: torch.float32\n",
      "layers.8.ln.bias: torch.float32\n",
      "layers.8.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.8.mixer.Wqkv.weight: torch.float32\n",
      "layers.8.mixer.Wqkv.bias: torch.float32\n",
      "layers.8.mixer.out_proj.weight: torch.float32\n",
      "layers.8.mixer.out_proj.bias: torch.float32\n",
      "layers.8.mlp.fc1.weight: torch.float32\n",
      "layers.8.mlp.fc1.bias: torch.float32\n",
      "layers.8.mlp.fc2.weight: torch.float32\n",
      "layers.8.mlp.fc2.bias: torch.float32\n",
      "layers.9.ln.weight: torch.float32\n",
      "layers.9.ln.bias: torch.float32\n",
      "layers.9.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.9.mixer.Wqkv.weight: torch.float32\n",
      "layers.9.mixer.Wqkv.bias: torch.float32\n",
      "layers.9.mixer.out_proj.weight: torch.float32\n",
      "layers.9.mixer.out_proj.bias: torch.float32\n",
      "layers.9.mlp.fc1.weight: torch.float32\n",
      "layers.9.mlp.fc1.bias: torch.float32\n",
      "layers.9.mlp.fc2.weight: torch.float32\n",
      "layers.9.mlp.fc2.bias: torch.float32\n",
      "layers.10.ln.weight: torch.float32\n",
      "layers.10.ln.bias: torch.float32\n",
      "layers.10.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.10.mixer.Wqkv.weight: torch.float32\n",
      "layers.10.mixer.Wqkv.bias: torch.float32\n",
      "layers.10.mixer.out_proj.weight: torch.float32\n",
      "layers.10.mixer.out_proj.bias: torch.float32\n",
      "layers.10.mlp.fc1.weight: torch.float32\n",
      "layers.10.mlp.fc1.bias: torch.float32\n",
      "layers.10.mlp.fc2.weight: torch.float32\n",
      "layers.10.mlp.fc2.bias: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for k, v in client_model.state_dict().items():\n",
    "    print(f\"{k}: {v.dtype}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.11.ln.weight: torch.float32\n",
      "layers.11.ln.bias: torch.float32\n",
      "layers.11.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.11.mixer.Wqkv.weight: torch.float32\n",
      "layers.11.mixer.Wqkv.bias: torch.float32\n",
      "layers.11.mixer.out_proj.weight: torch.float32\n",
      "layers.11.mixer.out_proj.bias: torch.float32\n",
      "layers.11.mlp.fc1.weight: torch.float32\n",
      "layers.11.mlp.fc1.bias: torch.float32\n",
      "layers.11.mlp.fc2.weight: torch.float32\n",
      "layers.11.mlp.fc2.bias: torch.float32\n",
      "layers.12.ln.weight: torch.float32\n",
      "layers.12.ln.bias: torch.float32\n",
      "layers.12.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.12.mixer.Wqkv.weight: torch.float32\n",
      "layers.12.mixer.Wqkv.bias: torch.float32\n",
      "layers.12.mixer.out_proj.weight: torch.float32\n",
      "layers.12.mixer.out_proj.bias: torch.float32\n",
      "layers.12.mlp.fc1.weight: torch.float32\n",
      "layers.12.mlp.fc1.bias: torch.float32\n",
      "layers.12.mlp.fc2.weight: torch.float32\n",
      "layers.12.mlp.fc2.bias: torch.float32\n",
      "layers.13.ln.weight: torch.float32\n",
      "layers.13.ln.bias: torch.float32\n",
      "layers.13.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.13.mixer.Wqkv.weight: torch.float32\n",
      "layers.13.mixer.Wqkv.bias: torch.float32\n",
      "layers.13.mixer.out_proj.weight: torch.float32\n",
      "layers.13.mixer.out_proj.bias: torch.float32\n",
      "layers.13.mlp.fc1.weight: torch.float32\n",
      "layers.13.mlp.fc1.bias: torch.float32\n",
      "layers.13.mlp.fc2.weight: torch.float32\n",
      "layers.13.mlp.fc2.bias: torch.float32\n",
      "layers.14.ln.weight: torch.float32\n",
      "layers.14.ln.bias: torch.float32\n",
      "layers.14.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.14.mixer.Wqkv.weight: torch.float32\n",
      "layers.14.mixer.Wqkv.bias: torch.float32\n",
      "layers.14.mixer.out_proj.weight: torch.float32\n",
      "layers.14.mixer.out_proj.bias: torch.float32\n",
      "layers.14.mlp.fc1.weight: torch.float32\n",
      "layers.14.mlp.fc1.bias: torch.float32\n",
      "layers.14.mlp.fc2.weight: torch.float32\n",
      "layers.14.mlp.fc2.bias: torch.float32\n",
      "layers.15.ln.weight: torch.float32\n",
      "layers.15.ln.bias: torch.float32\n",
      "layers.15.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.15.mixer.Wqkv.weight: torch.float32\n",
      "layers.15.mixer.Wqkv.bias: torch.float32\n",
      "layers.15.mixer.out_proj.weight: torch.float32\n",
      "layers.15.mixer.out_proj.bias: torch.float32\n",
      "layers.15.mlp.fc1.weight: torch.float32\n",
      "layers.15.mlp.fc1.bias: torch.float32\n",
      "layers.15.mlp.fc2.weight: torch.float32\n",
      "layers.15.mlp.fc2.bias: torch.float32\n",
      "layers.16.ln.weight: torch.float32\n",
      "layers.16.ln.bias: torch.float32\n",
      "layers.16.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.16.mixer.Wqkv.weight: torch.float32\n",
      "layers.16.mixer.Wqkv.bias: torch.float32\n",
      "layers.16.mixer.out_proj.weight: torch.float32\n",
      "layers.16.mixer.out_proj.bias: torch.float32\n",
      "layers.16.mlp.fc1.weight: torch.float32\n",
      "layers.16.mlp.fc1.bias: torch.float32\n",
      "layers.16.mlp.fc2.weight: torch.float32\n",
      "layers.16.mlp.fc2.bias: torch.float32\n",
      "layers.17.ln.weight: torch.float32\n",
      "layers.17.ln.bias: torch.float32\n",
      "layers.17.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.17.mixer.Wqkv.weight: torch.float32\n",
      "layers.17.mixer.Wqkv.bias: torch.float32\n",
      "layers.17.mixer.out_proj.weight: torch.float32\n",
      "layers.17.mixer.out_proj.bias: torch.float32\n",
      "layers.17.mlp.fc1.weight: torch.float32\n",
      "layers.17.mlp.fc1.bias: torch.float32\n",
      "layers.17.mlp.fc2.weight: torch.float32\n",
      "layers.17.mlp.fc2.bias: torch.float32\n",
      "layers.18.ln.weight: torch.float32\n",
      "layers.18.ln.bias: torch.float32\n",
      "layers.18.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.18.mixer.Wqkv.weight: torch.float32\n",
      "layers.18.mixer.Wqkv.bias: torch.float32\n",
      "layers.18.mixer.out_proj.weight: torch.float32\n",
      "layers.18.mixer.out_proj.bias: torch.float32\n",
      "layers.18.mlp.fc1.weight: torch.float32\n",
      "layers.18.mlp.fc1.bias: torch.float32\n",
      "layers.18.mlp.fc2.weight: torch.float32\n",
      "layers.18.mlp.fc2.bias: torch.float32\n",
      "layers.19.ln.weight: torch.float32\n",
      "layers.19.ln.bias: torch.float32\n",
      "layers.19.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.19.mixer.Wqkv.weight: torch.float32\n",
      "layers.19.mixer.Wqkv.bias: torch.float32\n",
      "layers.19.mixer.out_proj.weight: torch.float32\n",
      "layers.19.mixer.out_proj.bias: torch.float32\n",
      "layers.19.mlp.fc1.weight: torch.float32\n",
      "layers.19.mlp.fc1.bias: torch.float32\n",
      "layers.19.mlp.fc2.weight: torch.float32\n",
      "layers.19.mlp.fc2.bias: torch.float32\n",
      "layers.20.ln.weight: torch.float32\n",
      "layers.20.ln.bias: torch.float32\n",
      "layers.20.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.20.mixer.Wqkv.weight: torch.float32\n",
      "layers.20.mixer.Wqkv.bias: torch.float32\n",
      "layers.20.mixer.out_proj.weight: torch.float32\n",
      "layers.20.mixer.out_proj.bias: torch.float32\n",
      "layers.20.mlp.fc1.weight: torch.float32\n",
      "layers.20.mlp.fc1.bias: torch.float32\n",
      "layers.20.mlp.fc2.weight: torch.float32\n",
      "layers.20.mlp.fc2.bias: torch.float32\n",
      "layers.21.ln.weight: torch.float32\n",
      "layers.21.ln.bias: torch.float32\n",
      "layers.21.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.21.mixer.Wqkv.weight: torch.float32\n",
      "layers.21.mixer.Wqkv.bias: torch.float32\n",
      "layers.21.mixer.out_proj.weight: torch.float32\n",
      "layers.21.mixer.out_proj.bias: torch.float32\n",
      "layers.21.mlp.fc1.weight: torch.float32\n",
      "layers.21.mlp.fc1.bias: torch.float32\n",
      "layers.21.mlp.fc2.weight: torch.float32\n",
      "layers.21.mlp.fc2.bias: torch.float32\n",
      "layers.22.ln.weight: torch.float32\n",
      "layers.22.ln.bias: torch.float32\n",
      "layers.22.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.22.mixer.Wqkv.weight: torch.float32\n",
      "layers.22.mixer.Wqkv.bias: torch.float32\n",
      "layers.22.mixer.out_proj.weight: torch.float32\n",
      "layers.22.mixer.out_proj.bias: torch.float32\n",
      "layers.22.mlp.fc1.weight: torch.float32\n",
      "layers.22.mlp.fc1.bias: torch.float32\n",
      "layers.22.mlp.fc2.weight: torch.float32\n",
      "layers.22.mlp.fc2.bias: torch.float32\n",
      "layers.23.ln.weight: torch.float32\n",
      "layers.23.ln.bias: torch.float32\n",
      "layers.23.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.23.mixer.Wqkv.weight: torch.float32\n",
      "layers.23.mixer.Wqkv.bias: torch.float32\n",
      "layers.23.mixer.out_proj.weight: torch.float32\n",
      "layers.23.mixer.out_proj.bias: torch.float32\n",
      "layers.23.mlp.fc1.weight: torch.float32\n",
      "layers.23.mlp.fc1.bias: torch.float32\n",
      "layers.23.mlp.fc2.weight: torch.float32\n",
      "layers.23.mlp.fc2.bias: torch.float32\n",
      "layers.24.ln.weight: torch.float32\n",
      "layers.24.ln.bias: torch.float32\n",
      "layers.24.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.24.mixer.Wqkv.weight: torch.float32\n",
      "layers.24.mixer.Wqkv.bias: torch.float32\n",
      "layers.24.mixer.out_proj.weight: torch.float32\n",
      "layers.24.mixer.out_proj.bias: torch.float32\n",
      "layers.24.mlp.fc1.weight: torch.float32\n",
      "layers.24.mlp.fc1.bias: torch.float32\n",
      "layers.24.mlp.fc2.weight: torch.float32\n",
      "layers.24.mlp.fc2.bias: torch.float32\n",
      "layers.25.ln.weight: torch.float32\n",
      "layers.25.ln.bias: torch.float32\n",
      "layers.25.linear.weight: torch.float32\n",
      "layers.25.linear.bias: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for k, v in server_model.state_dict().items():\n",
    "    print(f\"{k}: {v.dtype}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for k, v in client_model.state_dict().items():\n",
    "#     torch.set_default_dtype(torch.float16)\n",
    "#     print(f\"{k}: {v.dtype}\")\n",
    "    \n",
    "# for k, v in client_model.state_dict().items():    \n",
    "#     print(f\"{k}: {v.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "client_model = client_model.to(client_device)\n",
    "server_model = server_model.to(server_device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.0.wte.weight: torch.float32\n",
      "layers.1.ln.weight: torch.float32\n",
      "layers.1.ln.bias: torch.float32\n",
      "layers.1.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.1.mixer.Wqkv.weight: torch.float32\n",
      "layers.1.mixer.Wqkv.bias: torch.float32\n",
      "layers.1.mixer.out_proj.weight: torch.float32\n",
      "layers.1.mixer.out_proj.bias: torch.float32\n",
      "layers.1.mlp.fc1.weight: torch.float32\n",
      "layers.1.mlp.fc1.bias: torch.float32\n",
      "layers.1.mlp.fc2.weight: torch.float32\n",
      "layers.1.mlp.fc2.bias: torch.float32\n",
      "layers.2.ln.weight: torch.float32\n",
      "layers.2.ln.bias: torch.float32\n",
      "layers.2.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.2.mixer.Wqkv.weight: torch.float32\n",
      "layers.2.mixer.Wqkv.bias: torch.float32\n",
      "layers.2.mixer.out_proj.weight: torch.float32\n",
      "layers.2.mixer.out_proj.bias: torch.float32\n",
      "layers.2.mlp.fc1.weight: torch.float32\n",
      "layers.2.mlp.fc1.bias: torch.float32\n",
      "layers.2.mlp.fc2.weight: torch.float32\n",
      "layers.2.mlp.fc2.bias: torch.float32\n",
      "layers.3.ln.weight: torch.float32\n",
      "layers.3.ln.bias: torch.float32\n",
      "layers.3.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.3.mixer.Wqkv.weight: torch.float32\n",
      "layers.3.mixer.Wqkv.bias: torch.float32\n",
      "layers.3.mixer.out_proj.weight: torch.float32\n",
      "layers.3.mixer.out_proj.bias: torch.float32\n",
      "layers.3.mlp.fc1.weight: torch.float32\n",
      "layers.3.mlp.fc1.bias: torch.float32\n",
      "layers.3.mlp.fc2.weight: torch.float32\n",
      "layers.3.mlp.fc2.bias: torch.float32\n",
      "layers.4.ln.weight: torch.float32\n",
      "layers.4.ln.bias: torch.float32\n",
      "layers.4.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.4.mixer.Wqkv.weight: torch.float32\n",
      "layers.4.mixer.Wqkv.bias: torch.float32\n",
      "layers.4.mixer.out_proj.weight: torch.float32\n",
      "layers.4.mixer.out_proj.bias: torch.float32\n",
      "layers.4.mlp.fc1.weight: torch.float32\n",
      "layers.4.mlp.fc1.bias: torch.float32\n",
      "layers.4.mlp.fc2.weight: torch.float32\n",
      "layers.4.mlp.fc2.bias: torch.float32\n",
      "layers.5.ln.weight: torch.float32\n",
      "layers.5.ln.bias: torch.float32\n",
      "layers.5.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.5.mixer.Wqkv.weight: torch.float32\n",
      "layers.5.mixer.Wqkv.bias: torch.float32\n",
      "layers.5.mixer.out_proj.weight: torch.float32\n",
      "layers.5.mixer.out_proj.bias: torch.float32\n",
      "layers.5.mlp.fc1.weight: torch.float32\n",
      "layers.5.mlp.fc1.bias: torch.float32\n",
      "layers.5.mlp.fc2.weight: torch.float32\n",
      "layers.5.mlp.fc2.bias: torch.float32\n",
      "layers.6.ln.weight: torch.float32\n",
      "layers.6.ln.bias: torch.float32\n",
      "layers.6.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.6.mixer.Wqkv.weight: torch.float32\n",
      "layers.6.mixer.Wqkv.bias: torch.float32\n",
      "layers.6.mixer.out_proj.weight: torch.float32\n",
      "layers.6.mixer.out_proj.bias: torch.float32\n",
      "layers.6.mlp.fc1.weight: torch.float32\n",
      "layers.6.mlp.fc1.bias: torch.float32\n",
      "layers.6.mlp.fc2.weight: torch.float32\n",
      "layers.6.mlp.fc2.bias: torch.float32\n",
      "layers.7.ln.weight: torch.float32\n",
      "layers.7.ln.bias: torch.float32\n",
      "layers.7.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.7.mixer.Wqkv.weight: torch.float32\n",
      "layers.7.mixer.Wqkv.bias: torch.float32\n",
      "layers.7.mixer.out_proj.weight: torch.float32\n",
      "layers.7.mixer.out_proj.bias: torch.float32\n",
      "layers.7.mlp.fc1.weight: torch.float32\n",
      "layers.7.mlp.fc1.bias: torch.float32\n",
      "layers.7.mlp.fc2.weight: torch.float32\n",
      "layers.7.mlp.fc2.bias: torch.float32\n",
      "layers.8.ln.weight: torch.float32\n",
      "layers.8.ln.bias: torch.float32\n",
      "layers.8.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.8.mixer.Wqkv.weight: torch.float32\n",
      "layers.8.mixer.Wqkv.bias: torch.float32\n",
      "layers.8.mixer.out_proj.weight: torch.float32\n",
      "layers.8.mixer.out_proj.bias: torch.float32\n",
      "layers.8.mlp.fc1.weight: torch.float32\n",
      "layers.8.mlp.fc1.bias: torch.float32\n",
      "layers.8.mlp.fc2.weight: torch.float32\n",
      "layers.8.mlp.fc2.bias: torch.float32\n",
      "layers.9.ln.weight: torch.float32\n",
      "layers.9.ln.bias: torch.float32\n",
      "layers.9.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.9.mixer.Wqkv.weight: torch.float32\n",
      "layers.9.mixer.Wqkv.bias: torch.float32\n",
      "layers.9.mixer.out_proj.weight: torch.float32\n",
      "layers.9.mixer.out_proj.bias: torch.float32\n",
      "layers.9.mlp.fc1.weight: torch.float32\n",
      "layers.9.mlp.fc1.bias: torch.float32\n",
      "layers.9.mlp.fc2.weight: torch.float32\n",
      "layers.9.mlp.fc2.bias: torch.float32\n",
      "layers.10.ln.weight: torch.float32\n",
      "layers.10.ln.bias: torch.float32\n",
      "layers.10.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.10.mixer.Wqkv.weight: torch.float32\n",
      "layers.10.mixer.Wqkv.bias: torch.float32\n",
      "layers.10.mixer.out_proj.weight: torch.float32\n",
      "layers.10.mixer.out_proj.bias: torch.float32\n",
      "layers.10.mlp.fc1.weight: torch.float32\n",
      "layers.10.mlp.fc1.bias: torch.float32\n",
      "layers.10.mlp.fc2.weight: torch.float32\n",
      "layers.10.mlp.fc2.bias: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for k, v in client_model.state_dict().items():\n",
    "    print(f\"{k}: {v.dtype}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "layers.11.ln.weight: torch.float32\n",
      "layers.11.ln.bias: torch.float32\n",
      "layers.11.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.11.mixer.Wqkv.weight: torch.float32\n",
      "layers.11.mixer.Wqkv.bias: torch.float32\n",
      "layers.11.mixer.out_proj.weight: torch.float32\n",
      "layers.11.mixer.out_proj.bias: torch.float32\n",
      "layers.11.mlp.fc1.weight: torch.float32\n",
      "layers.11.mlp.fc1.bias: torch.float32\n",
      "layers.11.mlp.fc2.weight: torch.float32\n",
      "layers.11.mlp.fc2.bias: torch.float32\n",
      "layers.12.ln.weight: torch.float32\n",
      "layers.12.ln.bias: torch.float32\n",
      "layers.12.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.12.mixer.Wqkv.weight: torch.float32\n",
      "layers.12.mixer.Wqkv.bias: torch.float32\n",
      "layers.12.mixer.out_proj.weight: torch.float32\n",
      "layers.12.mixer.out_proj.bias: torch.float32\n",
      "layers.12.mlp.fc1.weight: torch.float32\n",
      "layers.12.mlp.fc1.bias: torch.float32\n",
      "layers.12.mlp.fc2.weight: torch.float32\n",
      "layers.12.mlp.fc2.bias: torch.float32\n",
      "layers.13.ln.weight: torch.float32\n",
      "layers.13.ln.bias: torch.float32\n",
      "layers.13.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.13.mixer.Wqkv.weight: torch.float32\n",
      "layers.13.mixer.Wqkv.bias: torch.float32\n",
      "layers.13.mixer.out_proj.weight: torch.float32\n",
      "layers.13.mixer.out_proj.bias: torch.float32\n",
      "layers.13.mlp.fc1.weight: torch.float32\n",
      "layers.13.mlp.fc1.bias: torch.float32\n",
      "layers.13.mlp.fc2.weight: torch.float32\n",
      "layers.13.mlp.fc2.bias: torch.float32\n",
      "layers.14.ln.weight: torch.float32\n",
      "layers.14.ln.bias: torch.float32\n",
      "layers.14.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.14.mixer.Wqkv.weight: torch.float32\n",
      "layers.14.mixer.Wqkv.bias: torch.float32\n",
      "layers.14.mixer.out_proj.weight: torch.float32\n",
      "layers.14.mixer.out_proj.bias: torch.float32\n",
      "layers.14.mlp.fc1.weight: torch.float32\n",
      "layers.14.mlp.fc1.bias: torch.float32\n",
      "layers.14.mlp.fc2.weight: torch.float32\n",
      "layers.14.mlp.fc2.bias: torch.float32\n",
      "layers.15.ln.weight: torch.float32\n",
      "layers.15.ln.bias: torch.float32\n",
      "layers.15.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.15.mixer.Wqkv.weight: torch.float32\n",
      "layers.15.mixer.Wqkv.bias: torch.float32\n",
      "layers.15.mixer.out_proj.weight: torch.float32\n",
      "layers.15.mixer.out_proj.bias: torch.float32\n",
      "layers.15.mlp.fc1.weight: torch.float32\n",
      "layers.15.mlp.fc1.bias: torch.float32\n",
      "layers.15.mlp.fc2.weight: torch.float32\n",
      "layers.15.mlp.fc2.bias: torch.float32\n",
      "layers.16.ln.weight: torch.float32\n",
      "layers.16.ln.bias: torch.float32\n",
      "layers.16.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.16.mixer.Wqkv.weight: torch.float32\n",
      "layers.16.mixer.Wqkv.bias: torch.float32\n",
      "layers.16.mixer.out_proj.weight: torch.float32\n",
      "layers.16.mixer.out_proj.bias: torch.float32\n",
      "layers.16.mlp.fc1.weight: torch.float32\n",
      "layers.16.mlp.fc1.bias: torch.float32\n",
      "layers.16.mlp.fc2.weight: torch.float32\n",
      "layers.16.mlp.fc2.bias: torch.float32\n",
      "layers.17.ln.weight: torch.float32\n",
      "layers.17.ln.bias: torch.float32\n",
      "layers.17.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.17.mixer.Wqkv.weight: torch.float32\n",
      "layers.17.mixer.Wqkv.bias: torch.float32\n",
      "layers.17.mixer.out_proj.weight: torch.float32\n",
      "layers.17.mixer.out_proj.bias: torch.float32\n",
      "layers.17.mlp.fc1.weight: torch.float32\n",
      "layers.17.mlp.fc1.bias: torch.float32\n",
      "layers.17.mlp.fc2.weight: torch.float32\n",
      "layers.17.mlp.fc2.bias: torch.float32\n",
      "layers.18.ln.weight: torch.float32\n",
      "layers.18.ln.bias: torch.float32\n",
      "layers.18.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.18.mixer.Wqkv.weight: torch.float32\n",
      "layers.18.mixer.Wqkv.bias: torch.float32\n",
      "layers.18.mixer.out_proj.weight: torch.float32\n",
      "layers.18.mixer.out_proj.bias: torch.float32\n",
      "layers.18.mlp.fc1.weight: torch.float32\n",
      "layers.18.mlp.fc1.bias: torch.float32\n",
      "layers.18.mlp.fc2.weight: torch.float32\n",
      "layers.18.mlp.fc2.bias: torch.float32\n",
      "layers.19.ln.weight: torch.float32\n",
      "layers.19.ln.bias: torch.float32\n",
      "layers.19.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.19.mixer.Wqkv.weight: torch.float32\n",
      "layers.19.mixer.Wqkv.bias: torch.float32\n",
      "layers.19.mixer.out_proj.weight: torch.float32\n",
      "layers.19.mixer.out_proj.bias: torch.float32\n",
      "layers.19.mlp.fc1.weight: torch.float32\n",
      "layers.19.mlp.fc1.bias: torch.float32\n",
      "layers.19.mlp.fc2.weight: torch.float32\n",
      "layers.19.mlp.fc2.bias: torch.float32\n",
      "layers.20.ln.weight: torch.float32\n",
      "layers.20.ln.bias: torch.float32\n",
      "layers.20.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.20.mixer.Wqkv.weight: torch.float32\n",
      "layers.20.mixer.Wqkv.bias: torch.float32\n",
      "layers.20.mixer.out_proj.weight: torch.float32\n",
      "layers.20.mixer.out_proj.bias: torch.float32\n",
      "layers.20.mlp.fc1.weight: torch.float32\n",
      "layers.20.mlp.fc1.bias: torch.float32\n",
      "layers.20.mlp.fc2.weight: torch.float32\n",
      "layers.20.mlp.fc2.bias: torch.float32\n",
      "layers.21.ln.weight: torch.float32\n",
      "layers.21.ln.bias: torch.float32\n",
      "layers.21.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.21.mixer.Wqkv.weight: torch.float32\n",
      "layers.21.mixer.Wqkv.bias: torch.float32\n",
      "layers.21.mixer.out_proj.weight: torch.float32\n",
      "layers.21.mixer.out_proj.bias: torch.float32\n",
      "layers.21.mlp.fc1.weight: torch.float32\n",
      "layers.21.mlp.fc1.bias: torch.float32\n",
      "layers.21.mlp.fc2.weight: torch.float32\n",
      "layers.21.mlp.fc2.bias: torch.float32\n",
      "layers.22.ln.weight: torch.float32\n",
      "layers.22.ln.bias: torch.float32\n",
      "layers.22.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.22.mixer.Wqkv.weight: torch.float32\n",
      "layers.22.mixer.Wqkv.bias: torch.float32\n",
      "layers.22.mixer.out_proj.weight: torch.float32\n",
      "layers.22.mixer.out_proj.bias: torch.float32\n",
      "layers.22.mlp.fc1.weight: torch.float32\n",
      "layers.22.mlp.fc1.bias: torch.float32\n",
      "layers.22.mlp.fc2.weight: torch.float32\n",
      "layers.22.mlp.fc2.bias: torch.float32\n",
      "layers.23.ln.weight: torch.float32\n",
      "layers.23.ln.bias: torch.float32\n",
      "layers.23.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.23.mixer.Wqkv.weight: torch.float32\n",
      "layers.23.mixer.Wqkv.bias: torch.float32\n",
      "layers.23.mixer.out_proj.weight: torch.float32\n",
      "layers.23.mixer.out_proj.bias: torch.float32\n",
      "layers.23.mlp.fc1.weight: torch.float32\n",
      "layers.23.mlp.fc1.bias: torch.float32\n",
      "layers.23.mlp.fc2.weight: torch.float32\n",
      "layers.23.mlp.fc2.bias: torch.float32\n",
      "layers.24.ln.weight: torch.float32\n",
      "layers.24.ln.bias: torch.float32\n",
      "layers.24.mixer.rotary_emb.inv_freq: torch.float32\n",
      "layers.24.mixer.Wqkv.weight: torch.float32\n",
      "layers.24.mixer.Wqkv.bias: torch.float32\n",
      "layers.24.mixer.out_proj.weight: torch.float32\n",
      "layers.24.mixer.out_proj.bias: torch.float32\n",
      "layers.24.mlp.fc1.weight: torch.float32\n",
      "layers.24.mlp.fc1.bias: torch.float32\n",
      "layers.24.mlp.fc2.weight: torch.float32\n",
      "layers.24.mlp.fc2.bias: torch.float32\n",
      "layers.25.ln.weight: torch.float32\n",
      "layers.25.ln.bias: torch.float32\n",
      "layers.25.linear.weight: torch.float32\n",
      "layers.25.linear.bias: torch.float32\n"
     ]
    }
   ],
   "source": [
    "for k, v in server_model.state_dict().items():\n",
    "    print(f\"{k}: {v.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "lora_modules=[\"Wqkv\"] \n",
    "lora_config = LoraConfig(\n",
    "    r=1,  # dimension of the updated matrices\n",
    "    lora_alpha=64,  # parameter for scaling\n",
    "    target_modules=lora_modules,\n",
    "    lora_dropout=0.1,  # dropout probability for layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "client_model = get_peft_model(client_model, lora_config)\n",
    "server_model = get_peft_model(server_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 1150\n",
      "})\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f76c437e3a0>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator\n",
    "from data_utils import transform_data_to_fedml_format, group_texts, tokenize_function\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "# resize embeddings\n",
    "embedding_size = client_model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    client_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# load data\n",
    "\n",
    "block_size = tokenizer.model_max_length\n",
    "raw_datasets = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-raw-v1\",    \n",
    "    cache_dir=\"/app/.huggingface_cache/dataset/\",\n",
    "    streaming=False\n",
    ")\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "\n",
    "# data preprocessing \n",
    "__tokenize_function = partial(tokenize_function, text_column_name=\"text\", tokenizer=tokenizer)\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "                    __tokenize_function,\n",
    "                    batched=True,\n",
    "                    remove_columns=column_names,\n",
    "                    desc=\"Running tokenizer on dataset\",\n",
    "                )\n",
    "\n",
    "__group_texts = partial(group_texts, block_size=block_size)\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "                __group_texts,\n",
    "                batched=True,\n",
    "                # num_proc=1,\n",
    "                # load_from_cache_file=not data_args.overwrite_cache,\n",
    "                # desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "            )\n",
    "lm_datasets['train'].set_format(\"torch\", device=client_device)\n",
    "train_dataloader = DataLoader(lm_datasets['train'], shuffle=True, collate_fn=default_data_collator, batch_size=1)\n",
    "print(lm_datasets['train'])\n",
    "print(train_dataloader)\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "client_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in client_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in client_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "server_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in server_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in server_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "model_grouped_paramters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "server_optimizer = torch.optim.AdamW(server_grouped_parameters, lr=0.001)\n",
    "client_optimizer = torch.optim.AdamW(client_grouped_parameters, lr=0.001)\n",
    "model_optimizer = torch.optim.AdamW(model_grouped_paramters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---- trainable params of server_model ----\n",
      "base_model.model.layers.11.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.11.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.12.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.12.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.13.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.13.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.14.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.14.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.15.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.15.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.16.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.16.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.17.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.17.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.18.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.18.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.19.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.19.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.20.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.20.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.21.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.21.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.22.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.22.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.23.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.23.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.24.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.24.mixer.Wqkv.lora_B.default.weight True\n",
      "\n",
      "\n",
      "---- trainable params of client_model ----\n",
      "base_model.model.layers.1.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.1.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.2.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.2.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.3.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.3.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.4.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.4.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.5.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.5.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.6.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.6.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.7.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.7.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.8.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.8.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.9.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.9.mixer.Wqkv.lora_B.default.weight True\n",
      "base_model.model.layers.10.mixer.Wqkv.lora_A.default.weight True\n",
      "base_model.model.layers.10.mixer.Wqkv.lora_B.default.weight True\n",
      "\n",
      " 1150\n"
     ]
    }
   ],
   "source": [
    "def unfreeze_params(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "def print_trainable_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if(param.requires_grad):\n",
    "            print(name, param.requires_grad)\n",
    "        \n",
    "def print_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if(param.requires_grad):\n",
    "            print(name, param.grad)\n",
    "            \n",
    "# unfreeze_params(client_model)\n",
    "# unfreeze_params(server_model)\n",
    "print(\"\\n\\n---- trainable params of server_model ----\")\n",
    "print_trainable_params(server_model)\n",
    "print(\"\\n\\n---- trainable params of client_model ----\")\n",
    "print_trainable_params(client_model)\n",
    "print(f\"\\n {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1150\n",
      "step = 0 \n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 512.00 MiB (GPU 2; 10.76 GiB total capacity; 9.39 GiB already allocated; 429.56 MiB free; 9.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_model.ipynb Cell 15\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=27'>28</a>\u001b[0m \u001b[39m# print(client_model.dtype)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=28'>29</a>\u001b[0m \u001b[39m# print(server_model.dtype)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=30'>31</a>\u001b[0m client_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m input_ids, past_key_values, attention_mask, labels, acts \u001b[39m=\u001b[39m client_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mbatch)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m client_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_model.ipynb#X25sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m acts\u001b[39m.\u001b[39mretain_grad()\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/peft/peft_model.py:922\u001b[0m, in \u001b[0;36mPeftModelForCausalLM.forward\u001b[0;34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mforward in MPTForCausalLM does not support inputs_embeds\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    912\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_model(\n\u001b[1;32m    913\u001b[0m             input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    914\u001b[0m             attention_mask\u001b[39m=\u001b[39mattention_mask,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    919\u001b[0m             \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    920\u001b[0m         )\n\u001b[0;32m--> 922\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbase_model(\n\u001b[1;32m    923\u001b[0m         input_ids\u001b[39m=\u001b[39;49minput_ids,\n\u001b[1;32m    924\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m    925\u001b[0m         inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m    926\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    927\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    928\u001b[0m         output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m    929\u001b[0m         return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m    930\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    931\u001b[0m     )\n\u001b[1;32m    933\u001b[0m batch_size \u001b[39m=\u001b[39m input_ids\u001b[39m.\u001b[39mshape[\u001b[39m0\u001b[39m]\n\u001b[1;32m    934\u001b[0m \u001b[39mif\u001b[39;00m attention_mask \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    935\u001b[0m     \u001b[39m# concat prompt attention mask\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/pipe_mixformer_sequential.py:44\u001b[0m, in \u001b[0;36mClientSideMixFormerSequentialForCausalLM.forward\u001b[0;34m(self, input_ids, past_key_values, attention_mask, labels, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m hidden_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m0\u001b[39m](input_ids)\n\u001b[1;32m     43\u001b[0m \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlayers[\u001b[39m1\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msplit_layer]:  \u001b[39m# return intermediate tensor\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     hidden_layer \u001b[39m=\u001b[39m module(hidden_layer, past_key_values\u001b[39m=\u001b[39;49mpast_key_values, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m     45\u001b[0m \u001b[39mreturn\u001b[39;00m input_ids, past_key_values, attention_mask, labels, hidden_layer\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/modeling_mixformer_sequential.py:596\u001b[0m, in \u001b[0;36mParallelBlock.forward\u001b[0;34m(self, hidden_states, past_key_values, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    593\u001b[0m residual \u001b[39m=\u001b[39m hidden_states\n\u001b[1;32m    594\u001b[0m hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mln(hidden_states)\n\u001b[0;32m--> 596\u001b[0m attn_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmixer(hidden_states, past_key_values\u001b[39m=\u001b[39;49mpast_key_values, attention_mask\u001b[39m=\u001b[39;49mattention_mask)\n\u001b[1;32m    597\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(attn_outputs, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    598\u001b[0m     attn_outputs \u001b[39m=\u001b[39m attn_outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/modeling_mixformer_sequential.py:553\u001b[0m, in \u001b[0;36mMHA.forward\u001b[0;34m(self, x, past_key_values, attention_mask, cu_seqlens, max_seqlen, **kwargs)\u001b[0m\n\u001b[1;32m    551\u001b[0m         attn_output \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39minner_attn, qkv, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mattention_kwargs)\n\u001b[1;32m    552\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 553\u001b[0m         attn_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minner_attn(qkv, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mattention_kwargs)\n\u001b[1;32m    554\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    555\u001b[0m     q \u001b[39m=\u001b[39m qkv[:, :, \u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/phi-1_5/modeling_mixformer_sequential.py:304\u001b[0m, in \u001b[0;36mSelfAttention.forward\u001b[0;34m(self, qkv, causal, attention_mask, **kwargs)\u001b[0m\n\u001b[1;32m    301\u001b[0m     padding_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mfull((batch_size, seq_len), \u001b[39m-\u001b[39m\u001b[39m10000.0\u001b[39m, dtype\u001b[39m=\u001b[39mscores\u001b[39m.\u001b[39mdtype, device\u001b[39m=\u001b[39mscores\u001b[39m.\u001b[39mdevice)\n\u001b[1;32m    302\u001b[0m     padding_mask\u001b[39m.\u001b[39mmasked_fill_(attention_mask, \u001b[39m0.0\u001b[39m)\n\u001b[0;32m--> 304\u001b[0m     scores \u001b[39m=\u001b[39m scores \u001b[39m+\u001b[39;49m rearrange(padding_mask, \u001b[39m\"\u001b[39;49m\u001b[39mb s -> b 1 1 s\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m    306\u001b[0m \u001b[39mif\u001b[39;00m causal:\n\u001b[1;32m    307\u001b[0m     causal_mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtriu(torch\u001b[39m.\u001b[39mfull((seq_len, seq_len), \u001b[39m-\u001b[39m\u001b[39m10000.0\u001b[39m, device\u001b[39m=\u001b[39mscores\u001b[39m.\u001b[39mdevice), \u001b[39m1\u001b[39m)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 2; 10.76 GiB total capacity; 9.39 GiB already allocated; 429.56 MiB free; 9.51 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print_grad = False\n",
    "model.train()\n",
    "client_model.train()\n",
    "server_model.train()\n",
    "print(len(train_dataloader))\n",
    "\n",
    "latency = {\n",
    "    \"client\": [],\n",
    "    \"server\": [],\n",
    "    \"end-to-end\": [],\n",
    "    \"model\": []\n",
    "}\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(f\"step = {step} \")\n",
    "    \n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device=client_device)\n",
    "    \n",
    "    client_model, server_model = client_model.to(device=client_device), server_model.to(device=server_device)\n",
    "\n",
    "    # print(client_model.dtype)\n",
    "    # print(server_model.dtype)\n",
    "\n",
    "    client_start_time = time.perf_counter()\n",
    "    input_ids, past_key_values, attention_mask, labels, acts = client_model(**batch)\n",
    "    client_end_time = time.perf_counter()\n",
    "    \n",
    "    acts.retain_grad()\n",
    "    input_ids, attention_mask, labels, acts = input_ids.to(device=server_device), attention_mask.to(device=server_device), labels.to(device=server_device), acts.to(device=server_device)\n",
    "    \n",
    "    server_start_time = time.perf_counter()\n",
    "    split_outputs = server_model(input_ids=input_ids, past_key_values=past_key_values, attention_mask=attention_mask, labels=labels, hidden_layer_input=acts)\n",
    "    server_end_time = time.perf_counter()\n",
    "        \n",
    "    split_loss = split_outputs.loss\n",
    "    split_loss.backward()\n",
    "    split_perplexity = math.exp(split_loss)\n",
    "    \n",
    "    server_optimizer.step()\n",
    "    client_optimizer.step()\n",
    "    \n",
    "    # scaler.update()\n",
    "    if(print_grad):\n",
    "        print(\"\\n\\n---- grad on server_model ----\")\n",
    "        print_grad(server_model)    \n",
    "        print(\"\\n\\n---- grad on client_model ----\")\n",
    "        print_grad(client_model)\n",
    "    \n",
    "    latency[\"client\"].append(client_end_time-client_start_time)\n",
    "    latency[\"server\"].append(server_end_time-server_start_time)\n",
    "    latency[\"end-to-end\"].append(server_end_time-client_start_time)\n",
    "    print(f\"  - split_loss = {split_loss}\")\n",
    "    print(f\"  - split_perplexity = {split_perplexity}\")\n",
    "    print(f\"  - forward latency (sec): Client = {np.average(latency['client'])}  ||  Server = {np.average(latency['server'])}  || End-to-end = {np.average(latency['end-to-end'])}\")\n",
    "    \n",
    "    server_optimizer.zero_grad()\n",
    "    client_optimizer.zero_grad()\n",
    "    \n",
    "    # # Non-split training\n",
    "    # model = model.to(model_device)\n",
    "    # for key in batch.keys():\n",
    "    #     batch[key] = batch[key].to(model_device)\n",
    "    # model_start_time = time.perf_counter()\n",
    "    # model_output = model(**batch)\n",
    "    # model_end_time = time.perf_counter()\n",
    "    # model_loss = model_output.loss\n",
    "    \n",
    "    # model_perplexity = math.exp(model_loss)\n",
    "    # model_loss.backward()\n",
    "    # model_optimizer.step()\n",
    "    # model_optimizer.zero_grad()\n",
    "    # latency['model'].append(model_end_time-model_start_time)\n",
    "    # print(f\"  - model_loss = {model_loss}\")\n",
    "    # print(f\"  - model_perplexity = {model_perplexity}\")\n",
    "    # print(f\"  - Latency (sec): model = {np.average(latency['model'])}\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
