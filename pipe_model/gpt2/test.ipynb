{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For synchronous execution of CPU and GPU\n",
    "client_device = torch.device(\"cuda:2\")\n",
    "server_device = torch.device(\"cuda:1\")\n",
    "model_device = torch.device(\"cuda:0\")\n",
    "from transformers import OPTForQuestionAnswering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import GPT2LMHeadModel\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "\n",
    "class ClientSideGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "\n",
    "\n",
    "class ServerSideGPT2LMHeadModel(GPT2LMHeadModel):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        split_layer = 2\n",
    " \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OPTConfig\n",
    "config = OPTConfig(num_hidden_layers=1)\n",
    "client_model = ClientSideOPTForCausalLM(config)\n",
    "num_hidden_layers=11\n",
    "config = OPTConfig()\n",
    "server_model = ServerSideOPTForCausalLM(config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(server_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "lora_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\", \"lm_head\"] \n",
    "lora_config = LoraConfig(\n",
    "    r=1,  # dimension of the updated matrices\n",
    "    lora_alpha=64,  # parameter for scaling\n",
    "    target_modules=lora_modules,\n",
    "    lora_dropout=0.1,  # dropout probability for layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "client_model = get_peft_model(client_model, lora_config)\n",
    "server_model = get_peft_model(server_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(server_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(client_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator\n",
    "from data_utils import transform_data_to_fedml_format, group_texts, tokenize_function\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/opt-125m\", \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\"\n",
    ")\n",
    "inputs = tokenizer(\"Hello world!\",return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# resize embeddings\n",
    "embedding_size = client_model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    client_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# load data\n",
    "print(tokenizer)\n",
    "block_size = 1024\n",
    "raw_datasets = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-raw-v1\",\n",
    "    cache_dir=\"/app/.huggingface_cache/dataset/\",\n",
    "    streaming=False\n",
    ")\n",
    "print(raw_datasets)\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "print(column_names)\n",
    "\n",
    "# data preprocessing \n",
    "__tokenize_function = partial(tokenize_function, text_column_name='text', tokenizer=tokenizer)\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "                    __tokenize_function,\n",
    "                    batched=True,\n",
    "                    remove_columns=column_names,\n",
    "                    desc=\"Running tokenizer on dataset\",\n",
    "                )\n",
    "print(tokenized_datasets)\n",
    "\n",
    "__group_texts = partial(group_texts, block_size=block_size)\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "                __group_texts,\n",
    "                batched=True,\n",
    "                # num_proc=1,\n",
    "                # load_from_cache_file=not data_args.overwrite_cache,\n",
    "                # desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "            )\n",
    "\n",
    "print(lm_datasets['train'])\n",
    "lm_datasets['train'].set_format(\"torch\", device=client_device)\n",
    "train_dataloader = DataLoader(lm_datasets['train'], shuffle=True, collate_fn=default_data_collator, batch_size=1)\n",
    "print(train_dataloader)\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "client_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in client_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in client_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "server_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in server_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in server_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "model_grouped_paramters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "server_optimizer = torch.optim.AdamW(server_grouped_parameters, lr=0.001)\n",
    "client_optimizer = torch.optim.AdamW(client_grouped_parameters, lr=0.001)\n",
    "model_optimizer = torch.optim.AdamW(model_grouped_paramters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unfreeze_params(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "def print_trainable_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if(param.requires_grad):\n",
    "            print(name, param.requires_grad)\n",
    "        \n",
    "def print_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if(param.requires_grad):\n",
    "            print(name, param.grad)\n",
    "            \n",
    "# unfreeze_params(client_model)\n",
    "# unfreeze_params(server_model)\n",
    "print(\"\\n\\n---- trainable params of server_model ----\")\n",
    "print_trainable_params(server_model)\n",
    "print(\"\\n\\n---- trainable params of client_model ----\")\n",
    "print_trainable_params(client_model)\n",
    "print(f\"\\n {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print_grad = False\n",
    "model.train()\n",
    "client_model.train()\n",
    "server_model.train()\n",
    "print(len(train_dataloader))\n",
    "\n",
    "latency = {\n",
    "    \"client\": [],\n",
    "    \"server\": [],\n",
    "    \"end-to-end\": [],\n",
    "    \"model\": []\n",
    "}\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(f\"step = {step} \")\n",
    "    \n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device=client_device)\n",
    "    \n",
    "    client_model, server_model = client_model.to(dtype=torch.float16, device=client_device), server_model.to(dtype=torch.float16, device=server_device)\n",
    "    # Split training\n",
    "    # with torch.autocast(server_model.device.type, dtype=torch.float16, enabled=True):\n",
    "    client_start_time = time.perf_counter()\n",
    "    # input_ids, past_key_values, attention_mask, labels, acts = client_model(**batch)\n",
    "    split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict = client_model(**batch)\n",
    "    client_end_time = time.perf_counter()\n",
    "    split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict\n",
    "    \n",
    "    split_output = split_output.to(server_device)\n",
    "    # acts.retain_grad()\n",
    "    # input_ids, attention_mask, labels, acts = input_ids.to(device=server_device), attention_mask.to(device=server_device), labels.to(device=server_device), acts.to(device=server_device)\n",
    "    # split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict = split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict.to(server_device)\n",
    "    \n",
    "    server_start_time = time.perf_counter()\n",
    "    # split_outputs = server_model(input_ids=input_ids, past_key_values=past_key_values, attention_mask=attention_mask, labels=labels, hidden_layer_input=acts)\n",
    "    split_outputs = server_model()\n",
    "    server_end_time = time.perf_counter()\n",
    "        \n",
    "    split_loss = split_outputs.loss\n",
    "    # scaler.scale(split_loss).backward()\n",
    "    split_loss.backward()\n",
    "    split_perplexity = math.exp(split_loss)\n",
    "    \n",
    "    # scaler.step(server_optimizer)\n",
    "    # scaler.step(client_optimizer)\n",
    "    server_optimizer.step()\n",
    "    client_optimizer.step()\n",
    "    \n",
    "    # scaler.update()\n",
    "    if(print_grad):\n",
    "        print(\"\\n\\n---- grad on server_model ----\")\n",
    "        print_grad(server_model)    \n",
    "        print(\"\\n\\n---- grad on client_model ----\")\n",
    "        print_grad(client_model)\n",
    "    \n",
    "    latency[\"client\"].append(client_end_time-client_start_time)\n",
    "    latency[\"server\"].append(server_end_time-server_start_time)\n",
    "    latency[\"end-to-end\"].append(server_end_time-client_start_time)\n",
    "    print(f\"  - split_loss = {split_loss}\")\n",
    "    print(f\"  - split_perplexity = {split_perplexity}\")\n",
    "    print(f\"  - Latency (sec): Client = {np.average(latency['client'])}  ||  Server = {np.average(latency['server'])}  || End-to-end = {np.average(latency['end-to-end'])}\")\n",
    "    \n",
    "    server_optimizer.zero_grad()\n",
    "    client_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
