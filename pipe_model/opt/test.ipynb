{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.13.1+cu116\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import torch\n",
    "print(torch.__version__)\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "os.environ['CUDA_LAUNCH_BLOCKING'] = '1'  # For synchronous execution of CPU and GPU\n",
    "client_device = torch.device(\"cuda:2\")\n",
    "server_device = torch.device(\"cuda:1\")\n",
    "model_device = torch.device(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OPTForCausalLM\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers.modeling_outputs import CausalLMOutputWithPast\n",
    "\n",
    "\n",
    "class ClientSideOPTForCausalLM(OPTForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        \n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
    "        split_outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=inputs_embeds,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        # print(hidden_outputs)\n",
    "        return split_outputs, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict\n",
    "\n",
    "class ServerSideOPTForCausalLM(OPTForCausalLM):\n",
    "    def __init__(self, config):\n",
    "        super().__init__(config)\n",
    "        split_layer = 2\n",
    "        self.model.decoder.layers = self.model.decoder.layers[split_layer:]\n",
    "        self.model.decoder.embed_positions = None\n",
    "        self.model.decoder.embed_tokens = None\n",
    "        self.model.decoder.final_layer_norm = None\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        split_outputs: torch.Tensor,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        head_mask: Optional[torch.Tensor] = None,\n",
    "        past_key_values: Optional[List[torch.FloatTensor]] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        labels: Optional[torch.LongTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "    ):\n",
    "        \n",
    "        outputs = self.model.decoder(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            head_mask=head_mask,\n",
    "            past_key_values=past_key_values,\n",
    "            inputs_embeds=split_outputs,\n",
    "            use_cache=use_cache,\n",
    "            output_attentions=output_attentions,\n",
    "            output_hidden_states=output_hidden_states,\n",
    "            return_dict=return_dict,\n",
    "        )\n",
    "        logits = self.lm_head(outputs[0]).contiguous()\n",
    "\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # move labels to correct device to enable model parallelism\n",
    "            labels = labels.to(logits.device)\n",
    "            # Shift so that tokens < n predict n\n",
    "            shift_logits = logits[..., :-1, :].contiguous()\n",
    "            shift_labels = labels[..., 1:].contiguous()\n",
    "            # Flatten the tokens\n",
    "            loss_fct = CrossEntropyLoss()\n",
    "            loss = loss_fct(shift_logits.view(-1, self.config.vocab_size), shift_labels.view(-1))\n",
    "\n",
    "        if not return_dict:\n",
    "            output = (logits,) + outputs[1:]\n",
    "            return (loss,) + output if loss is not None else output\n",
    "\n",
    "        return CausalLMOutputWithPast(\n",
    "            loss=loss,\n",
    "            logits=logits,\n",
    "            past_key_values=outputs.past_key_values,\n",
    "            hidden_states=outputs.hidden_states,\n",
    "            attentions=outputs.attentions,\n",
    "        )\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import OPTConfig\n",
    "config = OPTConfig(num_hidden_layers=1)\n",
    "client_model = ClientSideOPTForCausalLM(config)\n",
    "num_hidden_layers=11\n",
    "config = OPTConfig()\n",
    "server_model = ServerSideOPTForCausalLM(config)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClientSideOPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "      (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "      (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (layers): ModuleList(\n",
      "        (0): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(client_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ServerSideOPTForCausalLM(\n",
      "  (model): OPTModel(\n",
      "    (decoder): OPTDecoder(\n",
      "      (embed_tokens): None\n",
      "      (embed_positions): None\n",
      "      (final_layer_norm): None\n",
      "      (layers): ModuleList(\n",
      "        (0): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (1): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (2): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (3): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (4): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (5): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (6): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (7): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (8): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "        (9): OPTDecoderLayer(\n",
      "          (self_attn): OPTAttention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (activation_fn): ReLU()\n",
      "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50272, bias=False)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(server_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import LoraConfig, get_peft_model\n",
    "import torch\n",
    "\n",
    "MODEL_NAME = \"facebook/opt-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    MODEL_NAME, \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\"\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\",\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "lora_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"out_proj\", \"fc1\", \"fc2\", \"lm_head\"] \n",
    "lora_config = LoraConfig(\n",
    "    r=1,  # dimension of the updated matrices\n",
    "    lora_alpha=64,  # parameter for scaling\n",
    "    target_modules=lora_modules,\n",
    "    lora_dropout=0.1,  # dropout probability for layers\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "client_model = get_peft_model(client_model, lora_config)\n",
    "server_model = get_peft_model(server_model, lora_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): OPTForCausalLM(\n",
      "      (model): OPTModel(\n",
      "        (decoder): OPTDecoder(\n",
      "          (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (layers): ModuleList(\n",
      "            (0): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (10): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (11): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(\n",
      "        in_features=768, out_features=50272, bias=False\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=1, out_features=50272, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ServerSideOPTForCausalLM(\n",
      "      (model): OPTModel(\n",
      "        (decoder): OPTDecoder(\n",
      "          (embed_tokens): None\n",
      "          (embed_positions): None\n",
      "          (final_layer_norm): None\n",
      "          (layers): ModuleList(\n",
      "            (0): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (1): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (2): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (3): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (4): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (5): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (6): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (7): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (8): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "            (9): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(\n",
      "        in_features=768, out_features=50272, bias=False\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=1, out_features=50272, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(server_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): ClientSideOPTForCausalLM(\n",
      "      (model): OPTModel(\n",
      "        (decoder): OPTDecoder(\n",
      "          (embed_tokens): Embedding(50272, 768, padding_idx=1)\n",
      "          (embed_positions): OPTLearnedPositionalEmbedding(2050, 768)\n",
      "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (layers): ModuleList(\n",
      "            (0): OPTDecoderLayer(\n",
      "              (self_attn): OPTAttention(\n",
      "                (k_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (v_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (q_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "                (out_proj): Linear(\n",
      "                  in_features=768, out_features=768, bias=True\n",
      "                  (lora_dropout): ModuleDict(\n",
      "                    (default): Dropout(p=0.1, inplace=False)\n",
      "                  )\n",
      "                  (lora_A): ModuleDict(\n",
      "                    (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                  )\n",
      "                  (lora_B): ModuleDict(\n",
      "                    (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                  )\n",
      "                  (lora_embedding_A): ParameterDict()\n",
      "                  (lora_embedding_B): ParameterDict()\n",
      "                )\n",
      "              )\n",
      "              (activation_fn): ReLU()\n",
      "              (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "              (fc1): Linear(\n",
      "                in_features=768, out_features=3072, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=3072, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (fc2): Linear(\n",
      "                in_features=3072, out_features=768, bias=True\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.1, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=3072, out_features=1, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=1, out_features=768, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "              )\n",
      "              (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (lm_head): Linear(\n",
      "        in_features=768, out_features=50272, bias=False\n",
      "        (lora_dropout): ModuleDict(\n",
      "          (default): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "        (lora_A): ModuleDict(\n",
      "          (default): Linear(in_features=768, out_features=1, bias=False)\n",
      "        )\n",
      "        (lora_B): ModuleDict(\n",
      "          (default): Linear(in_features=1, out_features=50272, bias=False)\n",
      "        )\n",
      "        (lora_embedding_A): ParameterDict()\n",
      "        (lora_embedding_B): ParameterDict()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(client_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input_ids': tensor([[    2, 31414,   232,   328]]), 'attention_mask': tensor([[1, 1, 1, 1]])}\n",
      "GPT2TokenizerFast(name_or_path='facebook/opt-125m', vocab_size=50265, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'eos_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'unk_token': AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=True), 'pad_token': AddedToken(\"<pad>\", rstrip=False, lstrip=False, single_word=False, normalized=True)}, clean_up_tokenization_spaces=True)\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "['text']\n",
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 4358\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 36718\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'attention_mask'],\n",
      "        num_rows: 3760\n",
      "    })\n",
      "})\n",
      "Dataset({\n",
      "    features: ['input_ids', 'attention_mask', 'labels'],\n",
      "    num_rows: 2355\n",
      "})\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7f41af1d7fa0>\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import default_data_collator\n",
    "from data_utils import transform_data_to_fedml_format, group_texts, tokenize_function\n",
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"facebook/opt-125m\", \n",
    "    trust_remote_code=True,\n",
    "    cache_dir=\"/app/.huggingface_cache/model/\"\n",
    ")\n",
    "inputs = tokenizer(\"Hello world!\",return_tensors=\"pt\")\n",
    "print(inputs)\n",
    "\n",
    "# resize embeddings\n",
    "embedding_size = client_model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    client_model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "\n",
    "# load data\n",
    "print(tokenizer)\n",
    "block_size = 1024\n",
    "raw_datasets = load_dataset(\n",
    "    \"wikitext\",\n",
    "    \"wikitext-2-raw-v1\",\n",
    "    cache_dir=\"/app/.huggingface_cache/dataset/\",\n",
    "    streaming=False\n",
    ")\n",
    "print(raw_datasets)\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "print(column_names)\n",
    "\n",
    "# data preprocessing \n",
    "__tokenize_function = partial(tokenize_function, text_column_name='text', tokenizer=tokenizer)\n",
    "tokenized_datasets = raw_datasets.map(\n",
    "                    __tokenize_function,\n",
    "                    batched=True,\n",
    "                    remove_columns=column_names,\n",
    "                    desc=\"Running tokenizer on dataset\",\n",
    "                )\n",
    "print(tokenized_datasets)\n",
    "\n",
    "__group_texts = partial(group_texts, block_size=block_size)\n",
    "lm_datasets = tokenized_datasets.map(\n",
    "                __group_texts,\n",
    "                batched=True,\n",
    "                # num_proc=1,\n",
    "                # load_from_cache_file=not data_args.overwrite_cache,\n",
    "                # desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "            )\n",
    "\n",
    "print(lm_datasets['train'])\n",
    "lm_datasets['train'].set_format(\"torch\", device=client_device)\n",
    "train_dataloader = DataLoader(lm_datasets['train'], shuffle=True, collate_fn=default_data_collator, batch_size=1)\n",
    "print(train_dataloader)\n",
    "no_decay = [\"bias\", \"layer_norm.weight\"]\n",
    "client_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in client_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in client_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "server_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in server_model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in server_model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "model_grouped_paramters = [\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.01,\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]\n",
    "server_optimizer = torch.optim.AdamW(server_grouped_parameters, lr=0.001)\n",
    "client_optimizer = torch.optim.AdamW(client_grouped_parameters, lr=0.001)\n",
    "model_optimizer = torch.optim.AdamW(model_grouped_paramters, lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "---- trainable params of server_model ----\n",
      "base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.1.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.1.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.1.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.1.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.1.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.2.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.2.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.2.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.2.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.2.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.3.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.3.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.3.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.3.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.3.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.4.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.4.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.4.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.4.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.4.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.5.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.5.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.5.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.5.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.5.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.6.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.6.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.6.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.6.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.6.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.7.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.7.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.7.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.7.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.7.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.8.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.8.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.8.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.8.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.8.fc2.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.9.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.9.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.9.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.9.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.9.fc2.lora_B.default.weight True\n",
      "base_model.model.lm_head.lora_A.default.weight True\n",
      "base_model.model.lm_head.lora_B.default.weight True\n",
      "\n",
      "\n",
      "---- trainable params of client_model ----\n",
      "base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.k_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.v_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.q_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.self_attn.out_proj.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc1.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc1.lora_B.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc2.lora_A.default.weight True\n",
      "base_model.model.model.decoder.layers.0.fc2.lora_B.default.weight True\n",
      "base_model.model.lm_head.lora_A.default.weight True\n",
      "base_model.model.lm_head.lora_B.default.weight True\n",
      "\n",
      " 2355\n"
     ]
    }
   ],
   "source": [
    "def unfreeze_params(model):\n",
    "    for param in model.parameters():\n",
    "        param.requires_grad = True\n",
    "        \n",
    "def print_trainable_params(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if(param.requires_grad):\n",
    "            print(name, param.requires_grad)\n",
    "        \n",
    "def print_grad(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if(param.requires_grad):\n",
    "            print(name, param.grad)\n",
    "            \n",
    "# unfreeze_params(client_model)\n",
    "# unfreeze_params(server_model)\n",
    "print(\"\\n\\n---- trainable params of server_model ----\")\n",
    "print_trainable_params(server_model)\n",
    "print(\"\\n\\n---- trainable params of client_model ----\")\n",
    "print_trainable_params(client_model)\n",
    "print(f\"\\n {len(train_dataloader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2355\n",
      "step = 0 \n",
      "BaseModelOutputWithPast(last_hidden_state=tensor([[[ 0.4541,  0.0764,  0.3213,  ...,  0.9365,  0.0616, -0.5015],\n",
      "         [-0.8564,  0.6431, -0.2213,  ...,  0.7637,  0.3423, -2.0781],\n",
      "         [-0.2100,  0.0800, -0.9648,  ...,  0.0854,  2.5332, -2.0527],\n",
      "         ...,\n",
      "         [ 0.3835,  0.0983,  0.2330,  ..., -0.0782,  0.1295,  0.1847],\n",
      "         [ 0.8315, -1.9922,  0.0691,  ...,  0.6812, -1.2275,  0.4658],\n",
      "         [-0.4324, -0.7661, -1.2197,  ..., -1.1074, -0.6133, -1.2627]]],\n",
      "       device='cuda:2', dtype=torch.float16, grad_fn=<NativeLayerNormBackward0>), past_key_values=((tensor([[[[ 7.9102e-01,  3.6670e-01, -8.7402e-01,  ...,  2.0557e-01,\n",
      "            9.8206e-02, -7.5439e-01],\n",
      "          [-5.7666e-01,  4.5068e-01, -4.8364e-01,  ..., -2.6514e-01,\n",
      "           -8.4326e-01, -1.0675e-01],\n",
      "          [-1.5625e-01,  7.1094e-01,  4.7913e-02,  ..., -1.1719e-01,\n",
      "           -3.4473e-01, -7.5537e-01],\n",
      "          ...,\n",
      "          [-2.8027e-01, -2.0544e-01,  1.4807e-01,  ...,  9.0674e-01,\n",
      "            1.1761e-01, -8.1104e-01],\n",
      "          [-2.6831e-01,  5.7812e-01,  5.2441e-01,  ..., -1.7065e-01,\n",
      "            3.1470e-01, -2.0593e-01],\n",
      "          [ 4.9634e-01,  4.3774e-01, -2.9736e-01,  ...,  4.5874e-01,\n",
      "            4.0967e-01, -8.1543e-01]],\n",
      "\n",
      "         [[-1.8787e-01, -7.8760e-01,  1.6687e-01,  ..., -3.6354e-03,\n",
      "           -1.6895e-01,  5.7587e-02],\n",
      "          [-3.8623e-01, -2.7002e-01,  5.1605e-02,  ..., -3.1616e-01,\n",
      "           -1.5356e-01,  4.6783e-02],\n",
      "          [-2.7783e-01,  5.3516e-01,  1.7700e-01,  ...,  3.1909e-01,\n",
      "           -6.0059e-01, -6.1670e-01],\n",
      "          ...,\n",
      "          [-3.0090e-02, -8.9404e-01, -1.0830e+00,  ...,  3.2617e-01,\n",
      "            1.1084e+00, -5.8740e-01],\n",
      "          [-6.0840e-01,  1.0760e-01, -2.8519e-02,  ..., -2.4090e-03,\n",
      "           -2.0605e-01, -7.4072e-01],\n",
      "          [ 1.7102e-01, -6.1371e-02,  3.6621e-01,  ...,  2.4582e-02,\n",
      "           -2.1191e-01, -8.8232e-01]],\n",
      "\n",
      "         [[-1.3501e-01, -5.8789e-01, -1.5526e-02,  ...,  5.5029e-01,\n",
      "           -2.1509e-01, -8.2568e-01],\n",
      "          [ 2.7393e-01,  1.4092e+00, -3.2013e-02,  ..., -1.5674e-01,\n",
      "           -7.0117e-01, -3.0737e-01],\n",
      "          [-4.4434e-01, -4.3304e-02, -4.8804e-01,  ..., -1.0729e-03,\n",
      "            6.1182e-01, -1.2031e+00],\n",
      "          ...,\n",
      "          [-5.8789e-01,  8.6121e-02, -2.2827e-01,  ..., -2.0862e-01,\n",
      "            2.0020e-01, -3.9502e-01],\n",
      "          [ 2.4268e-01,  6.0986e-01, -4.4165e-01,  ..., -3.8721e-01,\n",
      "            3.4619e-01, -6.1475e-01],\n",
      "          [ 5.1514e-01,  7.7490e-01,  8.8428e-01,  ...,  1.0364e-01,\n",
      "           -1.0626e-01, -5.5908e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.2510e-01,  4.8657e-01, -3.7231e-02,  ..., -5.6396e-01,\n",
      "            6.3379e-01, -4.9219e-01],\n",
      "          [ 1.9043e-01,  7.5623e-02,  3.1152e-01,  ...,  8.5889e-01,\n",
      "           -9.9182e-02,  1.2607e+00],\n",
      "          [-6.7578e-01, -3.9038e-01, -8.0566e-01,  ..., -4.2017e-01,\n",
      "            2.8442e-02,  3.8672e-01],\n",
      "          ...,\n",
      "          [-5.7617e-01, -1.1841e-01,  2.5146e-01,  ..., -5.4346e-01,\n",
      "            3.3325e-01, -6.5088e-01],\n",
      "          [-3.5278e-01,  1.3049e-01, -2.2607e-01,  ..., -9.5093e-02,\n",
      "            7.2021e-01,  3.2837e-01],\n",
      "          [-2.6520e-02, -2.4707e-01, -9.0576e-01,  ...,  4.0063e-01,\n",
      "            1.0234e+00, -1.7273e-01]],\n",
      "\n",
      "         [[ 2.8198e-01, -1.1406e+00, -9.3652e-01,  ..., -2.3840e-01,\n",
      "            7.0264e-01,  7.8320e-01],\n",
      "          [-2.8809e-01, -6.5430e-01, -2.7759e-01,  ...,  2.0752e-03,\n",
      "            2.2510e-01, -7.8369e-02],\n",
      "          [-5.3802e-02, -4.6411e-01,  2.5463e-03,  ...,  1.7419e-01,\n",
      "           -3.3057e-01,  8.1348e-01],\n",
      "          ...,\n",
      "          [ 2.7783e-01,  3.8501e-01,  5.9961e-01,  ..., -7.2119e-01,\n",
      "           -1.3965e-01, -8.8379e-01],\n",
      "          [ 4.9927e-01,  2.2217e-01, -1.8188e-01,  ..., -3.9136e-01,\n",
      "            5.5115e-02, -8.2581e-02],\n",
      "          [ 3.4204e-01,  1.0312e+00,  1.8396e-01,  ...,  1.1387e+00,\n",
      "           -6.5234e-01,  2.3889e-01]],\n",
      "\n",
      "         [[ 3.0103e-01, -3.4131e-01, -9.2090e-01,  ...,  3.0197e-02,\n",
      "           -2.6538e-01,  1.1772e-02],\n",
      "          [-2.7939e-02, -5.0049e-01,  1.6687e-01,  ...,  2.2461e-01,\n",
      "           -3.6401e-01,  6.7932e-02],\n",
      "          [-8.5352e-01,  3.6572e-01,  1.0547e+00,  ...,  1.9885e-01,\n",
      "            1.2656e+00, -1.2168e+00],\n",
      "          ...,\n",
      "          [-1.2769e-01,  4.5825e-01,  1.2461e+00,  ...,  7.0923e-02,\n",
      "            5.4077e-02,  7.7148e-02],\n",
      "          [-4.4580e-01, -4.8120e-01,  6.9580e-01,  ..., -1.1066e-01,\n",
      "            5.0342e-01, -1.4172e-01],\n",
      "          [-6.4636e-02, -3.6987e-01,  7.4268e-01,  ..., -2.0715e-01,\n",
      "           -2.4658e-02, -2.9663e-01]]]], device='cuda:2', dtype=torch.float16,\n",
      "       grad_fn=<CloneBackward0>), tensor([[[[-0.2600, -0.9590,  0.7583,  ...,  0.5386, -0.2520,  0.9014],\n",
      "          [ 0.1869, -1.6270,  0.9849,  ...,  0.9463,  0.0572,  0.2668],\n",
      "          [-0.1199,  0.3765, -0.9180,  ..., -0.6553, -0.4695,  0.7671],\n",
      "          ...,\n",
      "          [ 0.2573, -0.4163, -0.3081,  ..., -0.3442,  0.4285, -0.5527],\n",
      "          [ 0.0997,  0.6167,  0.3025,  ...,  1.0566, -0.3252, -1.0371],\n",
      "          [ 0.4824,  0.4512,  0.3699,  ...,  0.0997,  0.7402,  0.4800]],\n",
      "\n",
      "         [[ 1.2119,  0.4695,  0.7925,  ..., -0.4402, -0.8452, -0.5044],\n",
      "          [ 0.4919,  0.5391,  0.5459,  ...,  0.2683, -1.2969,  0.3506],\n",
      "          [-0.4971,  0.0771, -0.7544,  ...,  0.6846,  0.2656,  0.6064],\n",
      "          ...,\n",
      "          [ 0.5698,  1.3193,  0.5083,  ..., -0.5425,  0.5825, -0.5601],\n",
      "          [-0.3887, -0.2549,  0.4016,  ..., -0.1190, -0.0480, -0.0531],\n",
      "          [ 0.0084,  0.9219, -0.2252,  ..., -0.5010,  0.1135,  0.0248]],\n",
      "\n",
      "         [[ 1.2422,  0.2321,  0.0436,  ..., -0.0344, -0.3770,  0.5273],\n",
      "          [ 0.1300,  0.7861, -0.1998,  ...,  0.4709, -0.4041,  0.2937],\n",
      "          [ 0.5996, -0.2102, -0.0721,  ...,  0.6348,  0.0104, -0.1620],\n",
      "          ...,\n",
      "          [ 0.0094, -0.5635, -0.7236,  ...,  0.0453, -0.6196, -0.3406],\n",
      "          [ 0.8721, -0.1722,  0.0273,  ..., -1.0557,  0.6924,  0.5918],\n",
      "          [ 0.5234,  0.6118,  1.3447,  ..., -1.0049, -0.1711, -1.1025]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-0.1613,  0.4421,  0.5708,  ..., -0.2510,  0.9248,  0.0309],\n",
      "          [ 0.6831,  0.6660,  0.0048,  ..., -0.5205,  0.5474, -0.5132],\n",
      "          [-0.2622, -1.2363,  0.2878,  ...,  0.3625, -0.1344, -0.5537],\n",
      "          ...,\n",
      "          [ 0.1813, -0.4258, -0.5527,  ...,  0.0414,  0.0667,  0.1863],\n",
      "          [-0.0927, -0.4875,  0.1417,  ...,  0.4612,  0.0632, -0.3650],\n",
      "          [ 0.6743,  0.3123, -0.6885,  ..., -0.5952, -0.5586, -0.4790]],\n",
      "\n",
      "         [[ 0.4939,  0.0745, -0.4866,  ..., -0.3770, -0.4417, -0.7046],\n",
      "          [-0.3931, -0.5566, -0.1732,  ...,  0.8564,  0.5181,  0.1632],\n",
      "          [ 0.7031, -1.1230,  0.3960,  ..., -0.1610,  0.2561, -0.5981],\n",
      "          ...,\n",
      "          [-0.0320,  0.5771, -0.4688,  ...,  0.7114,  0.9326, -1.3936],\n",
      "          [ 0.6938,  0.1431, -1.0586,  ...,  0.0264, -0.4500, -0.3828],\n",
      "          [ 0.0162, -0.4224, -0.2654,  ..., -0.4836,  0.7510,  0.2573]],\n",
      "\n",
      "         [[ 0.3000,  1.0918, -0.5967,  ...,  0.0265, -0.0613, -0.8262],\n",
      "          [-0.8828, -0.0150, -0.1982,  ..., -0.3271,  0.0641, -1.0459],\n",
      "          [ 0.5093,  0.0342, -0.2416,  ...,  1.2852, -0.0174, -0.4370],\n",
      "          ...,\n",
      "          [ 1.1885,  0.5923,  0.0131,  ..., -0.1405, -0.4731,  0.0897],\n",
      "          [ 0.6118, -0.2646, -1.2266,  ...,  0.5566,  0.4785, -0.5229],\n",
      "          [ 1.1250,  0.6899, -0.1042,  ...,  0.0184, -0.0934, -0.0959]]]],\n",
      "       device='cuda:2', dtype=torch.float16, grad_fn=<CloneBackward0>)),), hidden_states=None, attentions=None)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'BaseModelOutputWithPast' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb Cell 12\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=31'>32</a>\u001b[0m client_end_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=32'>33</a>\u001b[0m split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict\n\u001b[0;32m---> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=34'>35</a>\u001b[0m split_output \u001b[39m=\u001b[39m split_output\u001b[39m.\u001b[39;49mto(server_device)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=35'>36</a>\u001b[0m \u001b[39m# acts.retain_grad()\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# input_ids, attention_mask, labels, acts = input_ids.to(device=server_device), attention_mask.to(device=server_device), labels.to(device=server_device), acts.to(device=server_device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=37'>38</a>\u001b[0m \u001b[39m# split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict = split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict.to(server_device)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f73706c69742d6c6561726e696e67227d@ssh-remote%2Blab-gpuserv3/app/example/mpi_torch_splitnn_cifar10_resnet56_example/pipe_model/opt/test.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=39'>40</a>\u001b[0m server_start_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mperf_counter()\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'BaseModelOutputWithPast' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "print_grad = False\n",
    "model.train()\n",
    "client_model.train()\n",
    "server_model.train()\n",
    "print(len(train_dataloader))\n",
    "\n",
    "latency = {\n",
    "    \"client\": [],\n",
    "    \"server\": [],\n",
    "    \"end-to-end\": [],\n",
    "    \"model\": []\n",
    "}\n",
    "\n",
    "\n",
    "scaler = torch.cuda.amp.GradScaler()\n",
    "for step, batch in enumerate(train_dataloader):\n",
    "    print(f\"step = {step} \")\n",
    "    \n",
    "    for key in batch.keys():\n",
    "        batch[key] = batch[key].to(device=client_device)\n",
    "    \n",
    "    client_model, server_model = client_model.to(dtype=torch.float16, device=client_device), server_model.to(dtype=torch.float16, device=server_device)\n",
    "    # Split training\n",
    "    # with torch.autocast(server_model.device.type, dtype=torch.float16, enabled=True):\n",
    "    client_start_time = time.perf_counter()\n",
    "    # input_ids, past_key_values, attention_mask, labels, acts = client_model(**batch)\n",
    "    split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict = client_model(**batch)\n",
    "    client_end_time = time.perf_counter()\n",
    "    split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict\n",
    "    \n",
    "    split_output = split_output.to(server_device)\n",
    "    # acts.retain_grad()\n",
    "    # input_ids, attention_mask, labels, acts = input_ids.to(device=server_device), attention_mask.to(device=server_device), labels.to(device=server_device), acts.to(device=server_device)\n",
    "    # split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict = split_output, attention_mask, head_mask, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict.to(server_device)\n",
    "    \n",
    "    server_start_time = time.perf_counter()\n",
    "    # split_outputs = server_model(input_ids=input_ids, past_key_values=past_key_values, attention_mask=attention_mask, labels=labels, hidden_layer_input=acts)\n",
    "    split_outputs = server_model()\n",
    "    server_end_time = time.perf_counter()\n",
    "        \n",
    "    split_loss = split_outputs.loss\n",
    "    # scaler.scale(split_loss).backward()\n",
    "    split_loss.backward()\n",
    "    split_perplexity = math.exp(split_loss)\n",
    "    \n",
    "    # scaler.step(server_optimizer)\n",
    "    # scaler.step(client_optimizer)\n",
    "    server_optimizer.step()\n",
    "    client_optimizer.step()\n",
    "    \n",
    "    # scaler.update()\n",
    "    if(print_grad):\n",
    "        print(\"\\n\\n---- grad on server_model ----\")\n",
    "        print_grad(server_model)    \n",
    "        print(\"\\n\\n---- grad on client_model ----\")\n",
    "        print_grad(client_model)\n",
    "    \n",
    "    latency[\"client\"].append(client_end_time-client_start_time)\n",
    "    latency[\"server\"].append(server_end_time-server_start_time)\n",
    "    latency[\"end-to-end\"].append(server_end_time-client_start_time)\n",
    "    print(f\"  - split_loss = {split_loss}\")\n",
    "    print(f\"  - split_perplexity = {split_perplexity}\")\n",
    "    print(f\"  - Latency (sec): Client = {np.average(latency['client'])}  ||  Server = {np.average(latency['server'])}  || End-to-end = {np.average(latency['end-to-end'])}\")\n",
    "    \n",
    "    server_optimizer.zero_grad()\n",
    "    client_optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
